{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required Binary modules: Numpy, Pandas, Seaborn, Matplotlib and other modules\n",
    "# These modules provide data processing, visualization and machine learning algorithm functions.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "# Set Palette and Drawing Style\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load dataset: Use the pd.read_csv() function to read data from a CSV file and convert it to DataFrame format.\n",
    "    poke = pd.read_csv('C:/Users/fortu/Downloads/pokemon0820 (2).csv')\n",
    "    test_total = poke.base_total.values\n",
    "    # Split the dataset: Divide the entire dataset into a training set and a test set\n",
    "    # 75% part of the data is used to train the program as well as the rest of it was used to test the result of machine learning\n",
    "    train = poke.iloc[:600, :]\n",
    "    test = poke.iloc[600:801, :]\n",
    "    # Display Dataset Information: Display number of rows and columns of the dataset\n",
    "    # check the number of dataset's rows and columns\n",
    "    print(\"About this dataset, it has {} rows and {} columns.\".format(poke.shape[0], poke.shape[1]))\n",
    "    # check the type of data\n",
    "    print(\"The type of POKE is {}\".format(type(poke)))\n",
    "\n",
    "    # Data Missing in the Dataset\n",
    "    poke.info()\n",
    "    # Basic description: including count, mean, standard deviation, minimum value and maximum value\n",
    "    print(poke.describe().T)\n",
    "    # Missing values for each attribute in the dashboard: Calculate the percentage of missing values for each attribute in the dataset and save the result in the variable missing_value_poke, including column names and the percentage of missing values.\n",
    "    percent_missing = poke.isnull().sum() * 100 / len(poke)\n",
    "    missing_value_poke = pd.DataFrame({\n",
    "        'column_name': poke.columns,\n",
    "        'percent_missing': percent_missing\n",
    "    })\n",
    "    # The code above is mainly used for loading, processing and analyzing data, including data description statistics, missing value detection and so on.\n",
    "\n",
    "    # Data preprocessing\n",
    "    (u, sigma) = norm.fit(poke['base_total'])\n",
    "    print('\\n u = {:.2f} and sigma = {:.2f}\\n'.format(u, sigma))\n",
    "    # # plot the distribution\n",
    "    # plt.hist(poke.base_total, bins=35)\n",
    "    # plt.xlabel('base_total')\n",
    "    # plt.ylabel('Frequency')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    res = stats.probplot(poke['base_total'], plot=plt)\n",
    "    # Data of \"base_total\" coincides with Normal Distribution. It could be more precise through log transformation.\n",
    "    poke['base_total'] = np.log1p(poke['base_total'])\n",
    "    # # check the new distribution\n",
    "    # get the fitted parameters used by the function\n",
    "    (u, sigma) = norm.fit(poke['base_total'])\n",
    "    print('\\n u = {:.2f} and sigma = {:.2f}\\n'.format(u, sigma))\n",
    "\n",
    "    # The Data Distribution is more like Normal Distribution through log transformation.\n",
    "\n",
    "    # Remove all sequences whose data type is \"object\", and get the index.\n",
    "    numeric_feats = poke.dtypes[poke.dtypes != \"object\"].index\n",
    "    # check the skew of all numerical features\n",
    "    skewed_feats = poke[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "    print(\"\\nSkew in numerical features: \\n\")\n",
    "    skewnewss = pd.DataFrame({'Skew': skewed_feats})\n",
    "    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewnewss.shape[0]))\n",
    "\n",
    "    from scipy.special import boxcox1p\n",
    "\n",
    "    skewed_features = skewnewss.index\n",
    "    lam = 0.15\n",
    "    for feat in skewed_features:\n",
    "        poke[feat] = boxcox1p(poke[feat], lam)\n",
    "    # print(skewnewss.head)\n",
    "\n",
    "    # Handle blank or missing values\n",
    "    y_train = train.base_total.values\n",
    "    y_test = test.base_total.values\n",
    "    all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "    all_data.drop(['base_total'], axis=1, inplace=True)\n",
    "    all_data.drop(['generation'], axis=1, inplace=True)\n",
    "    all_data.drop(['is_legendary'], axis=1, inplace=True)\n",
    "    print(\"all_data size is {}\".format(all_data.shape))\n",
    "    all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "    all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "    missing_data = pd.DataFrame({'Missing Ratio': all_data_na})\n",
    "\n",
    "    # Analyse Correlation\n",
    "    # Select and analyze 10 factors that may affect \"base_total\"\n",
    "    column = ['height_m', 'weight_kg', 'base_egg_steps', 'base_happiness', 'hp', 'attack', 'defense', 'sp_attack',\n",
    "              'sp_defense', 'speed', 'base_total']\n",
    "    # Imputation Missing Values\n",
    "    for i in range(len(column) - 1):\n",
    "        all_data[column[i]] = all_data[column[i]].fillna(0)\n",
    "    for i in ('name', 'japanese_name', 'pokedex_number', 'type1', 'type2', 'classfication', 'capture_rate', 'abilities',\n",
    "              'experience_growth', 'percentage_male'):\n",
    "        all_data.drop([i], axis=1, inplace=True)\n",
    "    against_column_index = poke.columns.tolist()[1:19]\n",
    "    for i in against_column_index:\n",
    "        all_data.drop([i], axis=1, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(len(column)):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        train[column[i]].hist(bins=100, color=color[0])\n",
    "        plt.xlabel(column[i], fontsize=10)\n",
    "        plt.ylabel('Frequency', fontsize=10)\n",
    "    plt.tight_layout() # subscheme generation\n",
    "    plt.show()\n",
    "\n",
    "    # BoxPlot\n",
    "    sns.set_style('ticks')\n",
    "    sns.set_context(\"notebook\", font_scale=1.1)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(len(column)):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.boxplot(x='base_total', y=column[i], data=train, color=color[0], width=0.6)\n",
    "        plt.ylabel(column[i], fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # HeatMap\n",
    "    sns.set_style(\"dark\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mcorr = train[column].corr()\n",
    "    mask = np.zeros_like(mcorr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    cmap = sns.diverging_palette(240, 20, as_cmap=True)\n",
    "    g = sns.heatmap(mcorr, mask=mask, cmap=cmap, square=True, annot=True, fmt='0.2f')\n",
    "\n",
    "    # Fit data by using Liner Regression models\n",
    "    sns.set_style('ticks')\n",
    "    sns.set_context(\"notebook\", font_scale=1.4)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.regplot(x='height_m', y='base_total', data=train, scatter_kws={'s': 5}, color=color[7])\n",
    "    sns.regplot(x='weight_kg', y='base_total', data=train, scatter_kws={'s': 5}, color=color[7])\n",
    "    sns.regplot(x='attack', y='base_total', data=train, scatter_kws={'s': 5}, color=color[7])\n",
    "    sns.regplot(x='sp_attack', y='base_total', data=train, scatter_kws={'s': 5}, color=color[7])\n",
    "    sns.regplot(x='defense', y='base_total', data=train, scatter_kws={'s': 5}, color=color[7])\n",
    "    sns.regplot(x='sp_defense', y='base_total', data=train, scatter_kws={'s': 5}, color=color[7])\n",
    "    sns.regplot(x='speed', y='base_total', data=train, scatter_kws={'s': 5}, color=color[7])\n",
    "    plt.show()\n",
    "\n",
    "    # Drawing Regression Image\n",
    "    sns.lmplot(x='base_total', y='defense', hue='sp_attack', data=poke, fit_reg=False, scatter_kws={'s': 10})\n",
    "    sns.set_style('ticks')\n",
    "    sns.set_context(\"notebook\", font_scale=1.4)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    cm = plt.cm.get_cmap('RdBu')\n",
    "    sc = plt.scatter(poke['sp_defense'], poke['base_total'], c=poke['sp_attack'], cmap=cm)\n",
    "    bar = plt.colorbar(sc)\n",
    "    bar.set_label('sp_attack', rotation=0)\n",
    "    plt.xlabel('sp_defense')\n",
    "    plt.ylabel('base_total')\n",
    "\n",
    "    # Feature Dimension Reduction\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA(n_components=9)\n",
    "    all_data_pca = pca.fit_transform(all_data)\n",
    "    ntrain = train.shape[0]\n",
    "    ntest = test.shape[0]\n",
    "    train = all_data[:ntrain]\n",
    "    test = all_data[ntrain:]\n",
    "\n",
    "    # Construct Machine Learning models\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, LassoLarsIC\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor\n",
    "    from sklearn.kernel_ridge import KernelRidge\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "    from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import xgboost as xgb\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    # validation function\n",
    "    n_folds = 5\n",
    "\n",
    "    def rmsle_cv(model):\n",
    "        kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "        rmse = np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "        return rmse\n",
    "\n",
    "    # different models\n",
    "    # Ordinary Least Squares\n",
    "    liner = make_pipeline(RobustScaler(), LinearRegression())\n",
    "    # Ridge\n",
    "    ridge = make_pipeline(RobustScaler(), Ridge(alpha=0.9, random_state=1))\n",
    "    # Lasso\n",
    "    lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\n",
    "    # ElasticNet\n",
    "    ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=1))\n",
    "    # Kernel Ridge\n",
    "    KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "    # calculate the error of each model mean(standard Deviation)\n",
    "    score = rmsle_cv(liner)\n",
    "    print(\"\\nLinearRregression score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "    score = rmsle_cv(ridge)\n",
    "    print(\"\\nRidge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "    score = rmsle_cv(lasso)\n",
    "    print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "    score = rmsle_cv(ENet)\n",
    "    print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "    score = rmsle_cv(KRR)\n",
    "    print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "    # Ensemble learning(contains Bagging, Boosting and Stacking)\n",
    "    # Random Forest\n",
    "    RandomForest = RandomForestRegressor(n_estimators=100, criterion='poisson')\n",
    "    # The 'criterion' parameter of RandomForestRegressor must be a str among {'friedman_mse', 'poisson', 'squared_error', 'absolute_error'}.\n",
    "    score = rmsle_cv(RandomForest)\n",
    "    print(\"RandomForest score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "    # GBDT\n",
    "    GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt',\n",
    "                                       min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=5)\n",
    "    # XGBoost\n",
    "    model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3,\n",
    "                                 min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8551,\n",
    "                                 subsample=0.5213, nthread=-1)\n",
    "    # LightGBM\n",
    "    model_lgb = lgb.LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.05, n_estimators=720,\n",
    "                                  max_bin=55, bagging_freq=5, feature_fraction=0.2319,\n",
    "                                  feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf=6,\n",
    "                                  min_sum_hessian_in_leaf=11)\n",
    "\n",
    "    # calculate the error of these models\n",
    "    score = rmsle_cv(GBoost)\n",
    "    print(\"GBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "    score = rmsle_cv(model_xgb)\n",
    "    print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "    score = rmsle_cv(model_lgb)\n",
    "    print(\"LGBMRegressorGBM score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "\n",
    "    # Define Metamodel classes\n",
    "    class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "        def __init__(self, base_models, meta_model, n_folds=5):\n",
    "            self.base_models = base_models\n",
    "            self.meta_model = meta_model\n",
    "            self.n_folds = n_folds\n",
    "\n",
    "        # We again fit the data on clones of the original models\n",
    "        def fit(self, X, y):\n",
    "            self.base_models_ = [list() for x in self.base_models]\n",
    "            self.meta_model_ = clone(self.meta_model)\n",
    "            kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "            # Train cloned base models then create out -of -fold predictions that are needed to train the cloned meta -model\n",
    "            out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "            for i, model in enumerate(self.base_models):\n",
    "                for train_index, holdout_index in kfold.split(X, y):\n",
    "                    instance = clone(model)\n",
    "                    self.base_models_[i].append(instance)\n",
    "                    instance.fit(X[train_index], y[train_index])\n",
    "                    y_pred = instance.predict(X[holdout_index])\n",
    "                    out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "            # Now train the cloned meta -model using the out -of -fold predictions as new feature\n",
    "            self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "            return self\n",
    "\n",
    "        # Do the predictions of all base models on the test data and use the averaged predictions as meta -features for the final prediction which is done by the meta -model\n",
    "        def predict(self, X):\n",
    "            meta_features = np.column_stack([\n",
    "                np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "                for base_models in self.base_models_])\n",
    "            return self.meta_model_.predict(meta_features)\n",
    "\n",
    "\n",
    "    stacked_averaged_models = StackingAveragedModels(base_models=(ridge, lasso, ENet),\n",
    "                                                     meta_model=liner)  # 0.0032 (0.0011) 0.002088808594979535\n",
    "    score = rmsle_cv(stacked_averaged_models)\n",
    "    print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "\n",
    "    # Loss function\n",
    "    def rmsle(y, y_pred):\n",
    "        return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "    # mean_squared_error (MSE) measures the amount of error in statistical models. It assesses the average squared difference between the observed and predicted values. When a model has no error, the MSE equals zero.\n",
    "    # Fit these data with StackingRegressor, XGBoost and LightGBM models.\n",
    "    stacked_averaged_models.fit(train.values, y_train)\n",
    "    stacked_train_predict = stacked_averaged_models.predict(train.values)\n",
    "    stacked_test_predict = stacked_averaged_models.predict(test)\n",
    "    stacked_poke_predict = stacked_averaged_models.predict(all_data)\n",
    "    print(\"stacked_averaged_models predicted data used for training: {}\".format(rmsle(y_train, stacked_train_predict)))\n",
    "\n",
    "    model_xgb.fit(train, y_train)\n",
    "    xgb_train_predict = model_xgb.predict(train)\n",
    "    xgb_test_predict = model_xgb.predict(test)\n",
    "    xgb_poke_predict = model_xgb.predict(all_data)\n",
    "    print(\"model_xgb predicted data used for training: {}\".format(rmsle(y_train, xgb_train_predict)))\n",
    "\n",
    "    GBoost.fit(train, y_train)\n",
    "    GBoost_train_predict = GBoost.predict(train)\n",
    "    GBoost_test_predict = GBoost.predict(test)\n",
    "    GBoost_poke_predict = GBoost.predict(all_data)\n",
    "    print(\"GBoost predicted data used for training: {}\".format(rmsle(y_train, GBoost_train_predict)))\n",
    "\n",
    "    KRR.fit(train, y_train)\n",
    "    KRR_train_predict = KRR.predict(train)\n",
    "    KRR_test_predict = KRR.predict(test)\n",
    "    KRR_poke_predict = KRR.predict(all_data)\n",
    "    print(\"KRR predicted data used for training: {}\".format(rmsle(y_train, KRR_train_predict)))\n",
    "\n",
    "    model_lgb.fit(train, y_train)\n",
    "    lgb_train_predict = model_lgb.predict(train)\n",
    "    lgb_test_predict = model_lgb.predict(test)\n",
    "    lgb_poke_predict = model_lgb.predict(all_data)\n",
    "    print(\"model_lgb predicted data used for training: {}\".format(rmsle(y_train, lgb_train_predict)))\n",
    "\n",
    "    # RMSE on the entire Train data when averaging\n",
    "    print('RMSLE score on train data')\n",
    "    print(rmsle(y_train, stacked_train_predict * 0.90 + KRR_train_predict * 0.10 + lgb_train_predict * 0.00))\n",
    "\n",
    "    # Results\n",
    "    ensemble = stacked_test_predict * 0.90 + KRR_test_predict * 0.10 + lgb_test_predict * 0.00\n",
    "    print(rmsle(y_test, ensemble))\n",
    "\n",
    "    # result = stacked_poke_predict * 0.90 + KRR_poke_predict * 0.10 + lgb_poke_predict * 0.00\n",
    "    result = stacked_poke_predict\n",
    "    print(rmsle(test_total, result))\n",
    "\n",
    "    res_file = pd.read_csv('C:/Users/fortu/Downloads/result.csv')\n",
    "    res_file['base_total'] = result\n",
    "    res_file.to_csv('sub.csv')\n",
    "\n",
    "    # The number of Pokemons in each generation.\n",
    "    res_file['generation'].value_counts().sort_values(ascending=False).plot.bar()\n",
    "\n",
    "    # The number of Pokemons in each lineage.\n",
    "    res_file['type1'].value_counts().sort_values(ascending=True).plot.barh()\n",
    "\n",
    "    # Distribution of different attributes——ViolinPlot\n",
    "    base_total = res_file.base_total\n",
    "    plt.subplots(figsize=(20, 12))\n",
    "    ax = sns.violinplot(x='type1', y='base_total', data=res_file, palette='muted')\n",
    "\n",
    "    # Find \"Ordinary Pokemon with excellent base_total\"\n",
    "    print(res_file[(res_file.base_total >= 570) & (res_file.is_legendary == 0)].japanese_name.head(10))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
